{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ce613c0",
   "metadata": {},
   "source": [
    "# Assignment 9\n",
    "The following will be going through running Naive Bayes, Logistic Regression, and Neural Networks on [a dataset over gender by name](https://archive.ics.uci.edu/ml/datasets/Gender+by+Name) using sklearn. This dataset was altered by trimming it down to 5000 entries, removing the probability and count column, as well as changing the M/F column to be 0/1 respectively.\n",
    "\n",
    "All of these algorithms are trying to identify male names. The data set is comprised of 41% male names total. Therefore, if the algorithms identify more than 41%, that means they are learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ee8d55",
   "metadata": {},
   "source": [
    "## General Preprocessing\n",
    "First things first though, we must process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "091c92a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows and columns: (4999, 2)\n",
      "      Name  Gender\n",
      "0    James       0\n",
      "1     John       0\n",
      "2   Robert       0\n",
      "3  Michael       0\n",
      "4  William       0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('name_gender_dataset_edited.csv', header=0, usecols=[0,1], encoding='latin-1')\n",
    "print('rows and columns:', df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b617d288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      James\n",
       "1       John\n",
       "2     Robert\n",
       "3    Michael\n",
       "4    William\n",
       "Name: Name, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "\n",
    "# set up X and y\n",
    "X = df.Name\n",
    "y = df.Gender\n",
    "\n",
    "# take a peek at X\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5b7113b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "5    1\n",
       "6    0\n",
       "7    0\n",
       "8    0\n",
       "9    0\n",
       "Name: Gender, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at y\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3d7cc7",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "The following will go over modelling Naive Bayes over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c804353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3999,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=1234)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11291459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: (3999, 3787)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "test size: (1000, 3787)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# apply tfidf vectorizer\n",
    "X_train = vectorizer.fit_transform(X_train)  # fit and transform the train data\n",
    "X_test = vectorizer.transform(X_test)        # transform only the test data\n",
    "\n",
    "# take a peek at the data\n",
    "# this is a very sparse matrix because most of the 8613 words don't occur in each sms message\n",
    "\n",
    "print('train size:', X_train.shape)\n",
    "print(X_train.toarray()[:5])\n",
    "\n",
    "print('\\ntest size:', X_test.shape)\n",
    "print(X_test.toarray()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe66db6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ee95264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior spam: 0.5926481620405101 log of prior: -0.5231543747176416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.5231543747176417"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# priors\n",
    "import math\n",
    "prior_p = sum(y_train == 1)/len(y_train)\n",
    "print('prior spam:', prior_p, 'log of prior:', math.log(prior_p))\n",
    "\n",
    "# the model prior matches the prior calculated above\n",
    "naive_bayes.class_log_prior_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bfb6f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.90359629, -8.59674347, -7.90359629, ..., -8.59674347,\n",
       "        -8.59674347, -8.59674347],\n",
       "       [-8.72518249, -8.03203531, -8.72518249, ..., -8.03203531,\n",
       "        -8.03203531, -8.03203531]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes.feature_log_prob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e569db69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 413]\n",
      " [ 52 535]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# make predictions on the test data\n",
    "pred = naive_bayes.predict(X_test)\n",
    "\n",
    "# print confusion matrix\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c2fdb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.535\n",
      "\n",
      "precision score (not spam):  0.0\n",
      "precision score (spam):  0.5643459915611815\n",
      "\n",
      "recall score: (not spam) 0.0\n",
      "recall score: (spam) 0.9114139693356048\n",
      "\n",
      "f1 score:  0.6970684039087949\n"
     ]
    }
   ],
   "source": [
    "print('accuracy score: ', accuracy_score(y_test, pred))\n",
    "      \n",
    "print('\\nprecision score (not spam): ', precision_score(y_test, pred, pos_label=0))\n",
    "print('precision score (spam): ', precision_score(y_test, pred))\n",
    "\n",
    "print('\\nrecall score: (not spam)', recall_score(y_test, pred, pos_label=0))\n",
    "print('recall score: (spam)', recall_score(y_test, pred))\n",
    "      \n",
    "print('\\nf1 score: ', f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "062125c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       413\n",
      "           1       0.56      0.91      0.70       587\n",
      "\n",
      "    accuracy                           0.54      1000\n",
      "   macro avg       0.28      0.46      0.35      1000\n",
      "weighted avg       0.33      0.54      0.41      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da70e258",
   "metadata": {},
   "source": [
    "The accuracy of the f1-score here is shown to be 54%, and because this is greater than the 41% of the original distribution, we can see that the Naive Bayes model actually learned and improved on its own, being able to guess 13% more accurately from what it learned. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25f2016",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "The following will go over modelling Logistic Regression over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5bea723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.535\n",
      "precision score:  0.5643459915611815\n",
      "recall score:  0.9114139693356048\n",
      "f1 score:  0.6970684039087949\n",
      "log loss:  0.7096596852017873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "\n",
    "#train\n",
    "classifier = LogisticRegression(solver='lbfgs', class_weight='balanced')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# evaluate\n",
    "pred = classifier.predict(X_test)\n",
    "print('accuracy score: ', accuracy_score(y_test, pred))\n",
    "print('precision score: ', precision_score(y_test, pred))\n",
    "print('recall score: ', recall_score(y_test, pred))\n",
    "print('f1 score: ', f1_score(y_test, pred))\n",
    "probs = classifier.predict_proba(X_test)\n",
    "print('log loss: ', log_loss(y_test, probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b7afb5",
   "metadata": {},
   "source": [
    "The accuracy is shown to be about 53% here, which is greater than the 41% of the original dataset. Because there is a 12% improvement, we can surmise that the logistic regression model learned and improved and was able to determine more about the dataset on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2267f1",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "The following will go over modelling Neural Networking over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1a96a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(15, 2), random_state=1,\n",
       "              solver='lbfgs')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifier = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                   hidden_layer_sizes=(15, 2), random_state=1)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a2e6640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.587\n",
      "precision score:  0.587\n",
      "recall score:  1.0\n",
      "f1 score:  0.7397605545053559\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "pred = classifier.predict(X_test)\n",
    "print('accuracy score: ', accuracy_score(y_test, pred))\n",
    "print('precision score: ', precision_score(y_test, pred))\n",
    "print('recall score: ', recall_score(y_test, pred))\n",
    "print('f1 score: ', f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8573d046",
   "metadata": {},
   "source": [
    "Finally, we have the neural network algorithm, and here we can see an even greater improvement from the past two models. This time, we have about 59%, which is a 18% improvement from the original datasets 41%. This shows that neural networking is perhaps the best way to learn and improve the accuracy for this particular dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c04fa25",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "After looking at all the results, I think its clear to see that the neural network algorithm worked the best here, though it sinteresting to see how the different algorithms worked and how they preformed differently. They all have their own unique uses and overall purposes, and none of them fully succeeded at the given task, however, it is clear that the neural network did the best at improving in overall accuracy, having a 18% improvement in comparision to a 13% or 12% improvement of Naive Bayes and Logistic Regression respectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
